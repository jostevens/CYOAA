{
    "contents" : "Linear Regression Models\n========================================================\n\nLinear Regession is great, the statistical workhorse of social science, and good deal of other sciences as well. If you ever took an introductory statistics course in college you prbably ended the class with this type of model.\n\nRegression is a model that helps us to understand how much different variables (like the amount of sunshine and temperature) move together. I would expect that if there is more sunlight shining through my window, the warmer it will be inside my house. Regression allows us to look at how much warmer it will be if we get an hour more sunlight. We can also extend the method to figure out how the amount of warmth changes based on the time of year (so if we get the same total amount of sunshine in March as we do in September how much does the season effect how hot it is).\n\nSome practical examples would be if you wanted to see how much end of bonuses increased productivity (manager), or how much more chicken needs to be cooked to feed 5 more 10 year olds coming for a birthday party (parent), or how much the demand for copper increases the price of copper (analyst).\n\nThe basic way it works is to imagine that you have two variables that you think are related. Make a scatterplot of the two variables and then try to draw a straight line in such a way that it gets as close as possible to most of the points.\n\nIn the figure below I'll do this with three data points. With these three points of data we can see that we have a line, but the line isn't really that close to the data. Lets try again with 500 data points.\n\n\n```{r lm.1, fig.height=6, fig.width=6, echo=FALSE}\nset.seed(2340)\nvar1 <- rnorm(500, 50, 2.5)\nvar2 <- rnorm(500, 2, 5)\nvar3 <- rnorm(500, 3, 3)\ndata1 <- data.frame(d1, d2, d3)\n\nplot(data1[1:3,1:2], main=\"Scatterplot of Data with 3 data points\", ylab=\"Variable 1\", xlab=\"Variable 2\")\nabline(lm(d2~d1, data1[1:3,]), col=\"red\")\n\nplot(data1[,1:2], main=\"Scatterplot of Data with 500 data points\", ylab=\"Variable 1\", xlab=\"Variable 2\")\nabline(lm(d2~d1, data1), col=\"red\")\n```\n\nWith more data it is a bit easier to see that the line goes through the middle of the cloud of data points. In reality it is getting as close as possible to everydata point while staying perfectly straight. \n\n\nAssumptions\n------------\nThere are a number of assumptions in linear regression, the nice thing is that they are very weak assumptions. So If you break the assumptions you will probably still get decent results. Here is the list of assumptions:\n* Normal Distribution\n* Independence\n* Linearity\n* Homoskedactity\n\nLet's look at each of these assumptions.\n\nNormal Distribution. In a regression it is assumed that the errors of the model are normaly distributed.\n\nIndependence. The value of any one data point\n\nCode\n----------\n\nIn R the way to perform linear regression is with the command `lm()`, for linear model. I'm going to use the same dataset that I used to make the scatterplots above which I called `data1` and it has variables named var1, var2, and var3.\n```{r, echo=TRUE}\n\nlm(var1 ~ var2, data=data1)\n\n```\n\n\nGoing Deeper\n--------------\n\n\n[1] I'm totally making this data up. You can see the code on github.\n",
    "created" : 1397243556875.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1885780191",
    "id" : "A0B0B218",
    "lastKnownWriteTime" : 1387169876,
    "path" : "C:/Dropbox/school/Working Papers/COYDA/Code/LinearRegression.Rmd",
    "project_path" : "LinearRegression.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}